#!/bin/bash
# Qwen2VL æœ¬åœ°å®ç°å®‰è£…è„šæœ¬
# åœ¨æœ‰ GPU çš„æœºå™¨ä¸Šè¿è¡Œæ­¤è„šæœ¬

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

echo "========================================"
echo "Qwen2VL æœ¬åœ°å®ç°å®‰è£…è„šæœ¬"
echo "========================================"
echo ""

# é¢œè‰²å®šä¹‰
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

# æ£€æŸ¥æ˜¯å¦åœ¨æ­£ç¡®çš„ç›®å½•
if [ ! -f "setup.py" ] && [ ! -f "pyproject.toml" ]; then
    echo -e "${RED}é”™è¯¯ï¼šè¯·åœ¨ lmms-eval æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬${NC}"
    exit 1
fi

echo -e "${YELLOW}[1/5] æ£€æŸ¥ Python ç¯å¢ƒ...${NC}"
python --version
if [ $? -ne 0 ]; then
    echo -e "${RED}é”™è¯¯ï¼šPython æœªå®‰è£…æˆ–ä¸åœ¨ PATH ä¸­${NC}"
    exit 1
fi
echo -e "${GREEN}âœ“ Python ç¯å¢ƒæ­£å¸¸${NC}"
echo ""

echo -e "${YELLOW}[2/5] å®‰è£… lmms-eval (editable æ¨¡å¼)...${NC}"
pip install -e .
if [ $? -ne 0 ]; then
    echo -e "${RED}é”™è¯¯ï¼šlmms-eval å®‰è£…å¤±è´¥${NC}"
    exit 1
fi
echo -e "${GREEN}âœ“ lmms-eval å®‰è£…æˆåŠŸ${NC}"
echo ""

echo -e "${YELLOW}[3/5] å®‰è£…å¿…è¦ä¾èµ–...${NC}"
pip install transformers torch accelerate qwen-vl-utils decord pillow numpy requests
if [ $? -ne 0 ]; then
    echo -e "${RED}é”™è¯¯ï¼šä¾èµ–å®‰è£…å¤±è´¥${NC}"
    exit 1
fi
echo -e "${GREEN}âœ“ ä¾èµ–å®‰è£…æˆåŠŸ${NC}"
echo ""

echo -e "${YELLOW}[4/5] éªŒè¯æœ¬åœ°å®ç°...${NC}"
echo "è¿è¡Œæµ‹è¯•è„šæœ¬..."
python -c "
import sys
try:
    from lmms_eval.models.local_models.qwen2_vl.modeling_qwen2_vl import Qwen2VLForConditionalGeneration
    from lmms_eval.models.local_models.qwen2_vl.processing_qwen2_vl import Qwen2VLProcessor
    print('âœ“ æœ¬åœ° Qwen2VL å®ç°å¯¼å…¥æˆåŠŸ')
    sys.exit(0)
except Exception as e:
    print(f'âœ— å¯¼å…¥å¤±è´¥: {e}')
    sys.exit(1)
"
if [ $? -ne 0 ]; then
    echo -e "${RED}é”™è¯¯ï¼šæœ¬åœ°å®ç°éªŒè¯å¤±è´¥${NC}"
    exit 1
fi
echo -e "${GREEN}âœ“ æœ¬åœ°å®ç°éªŒè¯é€šè¿‡${NC}"
echo ""

echo -e "${YELLOW}[5/5] æ£€æŸ¥ GPU å¯ç”¨æ€§...${NC}"
python -c "
import torch
if torch.cuda.is_available():
    print(f'âœ“ GPU å¯ç”¨ï¼š{torch.cuda.get_device_name(0)}')
    print(f'  GPU æ•°é‡: {torch.cuda.device_count()}')
    print(f'  CUDA ç‰ˆæœ¬: {torch.version.cuda}')
else:
    print('âš ï¸  è­¦å‘Šï¼šæœªæ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨ CPUï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰')
"
echo ""

echo "========================================"
echo -e "${GREEN}å®‰è£…å®Œæˆï¼${NC}"
echo "========================================"
echo ""
echo "ä¸‹ä¸€æ­¥ï¼š"
echo ""
echo "1. è¿è¡Œå¿«é€Ÿæµ‹è¯•ï¼ˆå¯é€‰ï¼‰ï¼š"
echo "   ${YELLOW}python test_local_qwen2vl.py${NC}"
echo ""
echo "2. è¿è¡Œå®Œæ•´è¯„æµ‹éªŒè¯ï¼š"
echo "   ${YELLOW}lmms-eval --model qwen2_vl --model_args pretrained=Qwen/Qwen2-VL-7B-Instruct --tasks mme --batch_size 1${NC}"
echo ""
echo "3. æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£ï¼š"
echo "   ${YELLOW}cat QUICK_START_LOCAL_QWEN2VL.md${NC}"
echo ""
echo "æœŸæœ›çœ‹åˆ°çš„éªŒè¯ä¿¡æ¯ï¼š"
echo "   ${GREEN}ğŸ”¥ Using LOCAL Qwen2VL implementation from lmms_eval/models/local_models/qwen2_vl/ ğŸ”¥${NC}"
echo ""
echo "========================================"


