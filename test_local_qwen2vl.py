#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•è„šæœ¬ï¼šéªŒè¯æœ¬åœ° Qwen2VL å®ç°æ˜¯å¦æ­£å¸¸å·¥ä½œ

ä½¿ç”¨æ–¹æ³•ï¼š
    python test_local_qwen2vl.py

é¢„æœŸè¾“å‡ºï¼š
    - åº”è¯¥çœ‹åˆ° "ğŸ”¥ Using LOCAL Qwen2VL implementation..." çš„æ‰“å°
    - æ¨¡å‹èƒ½å¤ŸæˆåŠŸåŠ è½½å¹¶ç”Ÿæˆè¾“å‡º
"""

import torch
from PIL import Image
import requests
from io import BytesIO

# å¯¼å…¥æœ¬åœ°å®ç°
print("=" * 80)
print("æµ‹è¯•æœ¬åœ° Qwen2VL å®ç°")
print("=" * 80)

print("\n[1/4] å¯¼å…¥æœ¬åœ° Qwen2VL å®ç°...")
try:
    from lmms_eval.models.local_models.qwen2_vl.modeling_qwen2_vl import Qwen2VLForConditionalGeneration
    from lmms_eval.models.local_models.qwen2_vl.processing_qwen2_vl import Qwen2VLProcessor
    print("âœ“ æˆåŠŸå¯¼å…¥æœ¬åœ°å®ç°")
except Exception as e:
    print(f"âœ— å¯¼å…¥å¤±è´¥: {e}")
    print("\nè¯·ç¡®ä¿å·²è¿è¡Œ: pip install -e .")
    exit(1)

print("\n[2/4] åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨...")
model_name = "Qwen/Qwen2-VL-7B-Instruct"
try:
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        model_name,
        torch_dtype="auto",
        device_map="auto"
    )
    processor = Qwen2VLProcessor.from_pretrained(model_name)
    print(f"âœ“ æˆåŠŸåŠ è½½æ¨¡å‹: {model_name}")
except Exception as e:
    print(f"âœ— åŠ è½½å¤±è´¥: {e}")
    exit(1)

print("\n[3/4] å‡†å¤‡æµ‹è¯•å›¾åƒå’Œæ–‡æœ¬...")
try:
    # ä½¿ç”¨ä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒ
    url = "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"
    response = requests.get(url)
    image = Image.open(BytesIO(response.content))
    print("âœ“ æˆåŠŸåŠ è½½æµ‹è¯•å›¾åƒ")
    
    # å‡†å¤‡æ¶ˆæ¯
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": "What is in this image?"}
            ]
        }
    ]
    print("âœ“ å‡†å¤‡å¥½æµ‹è¯•æ¶ˆæ¯")
except Exception as e:
    print(f"âœ— å‡†å¤‡å¤±è´¥: {e}")
    exit(1)

print("\n[4/4] è¿è¡Œæ¨ç†...")
print("-" * 80)
print("âš ï¸  æ³¨æ„ï¼šä½ åº”è¯¥ä¼šçœ‹åˆ°ä¸‹é¢çš„éªŒè¯ä¿¡æ¯ï¼š")
print("   'ğŸ”¥ Using LOCAL Qwen2VL implementation from lmms_eval/models/local_models/qwen2_vl/ ğŸ”¥'")
print("-" * 80)

try:
    # å¤„ç†è¾“å…¥
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä½¿ç”¨ qwen_vl_utils.process_vision_info
    try:
        from qwen_vl_utils import process_vision_info
        image_inputs, video_inputs = process_vision_info(messages)
    except ImportError:
        print("\nâš ï¸  è­¦å‘Šï¼šqwen_vl_utils æœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ–å¤„ç†")
        print("   è¯·è¿è¡Œ: pip install qwen-vl-utils")
        image_inputs = [image]
        video_inputs = None
    
    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt"
    ).to(model.device)
    
    # ç”Ÿæˆè¾“å‡ºï¼ˆè¿™é‡Œä¼šè§¦å‘ forwardï¼Œåº”è¯¥èƒ½çœ‹åˆ°æˆ‘ä»¬çš„ printï¼‰
    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=128
        )
    
    # è§£ç è¾“å‡º
    generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )
    
    print("-" * 80)
    print("âœ“ æ¨ç†æˆåŠŸå®Œæˆï¼")
    print(f"\nç”Ÿæˆçš„è¾“å‡º: {output_text[0]}")
    
except Exception as e:
    print(f"âœ— æ¨ç†å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    exit(1)

print("\n" + "=" * 80)
print("æµ‹è¯•å®Œæˆï¼")
print("=" * 80)
print("\næ€»ç»“ï¼š")
print("1. å¦‚æœä½ çœ‹åˆ°äº† 'ğŸ”¥ Using LOCAL...' çš„è¾“å‡ºï¼Œè¯´æ˜æœ¬åœ°å®ç°å·²ç»ç”Ÿæ•ˆ")
print("2. å¦‚æœæ¨¡å‹æˆåŠŸç”Ÿæˆäº†è¾“å‡ºï¼Œè¯´æ˜æœ¬åœ°å®ç°å·¥ä½œæ­£å¸¸")
print("3. ç°åœ¨ä½ å¯ä»¥ä¿®æ”¹ modeling_qwen2_vl.py æ¥æ”¹å˜æ¨ç†è¡Œä¸º")
print("\nä¸‹ä¸€æ­¥ï¼šè¿è¡Œå®Œæ•´çš„ lmms-eval è¯„æµ‹ï¼Œå¯¹æ¯”ç»“æœæ˜¯å¦ä¸€è‡´")
print("=" * 80)


